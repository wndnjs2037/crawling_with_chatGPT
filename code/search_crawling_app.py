import streamlit as st
import pandas as pd
from newspaper import Config, Article
import googletrans
import openai
from langdetect import detect
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import re
from selenium import webdriver
from youtube_transcript_api import YouTubeTranscriptApi
from konlpy.tag import Kkma
from pykospacing import Spacing
from playwright.sync_api import sync_playwright
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time
import urllib.request
import json
import urllib
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

# í•„ìš”í•œ ì „ì—­ë³€ìˆ˜ ì„ ì–¸
user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
config = Config()
translator = googletrans.Translator()
input_prompt = ""
first_url = ""
second_url = ""
third_url = ""

# openAPI ê°œì¸ìš© ì‹œí¬ë¦¿í‚¤
YOUR_API_KEY = 'ë³´ì•ˆìƒì˜ ì´ìœ ë¡œ ê¹ƒí—ˆë¸Œì—ëŠ” ì˜¬ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤'

#  chatGPTì— ëª…ë ¹ì–´ë¥¼ ë‚ ë¦¬ê¸° ìœ„í•œ í•¨ìˆ˜
def chatGPT(prompt, API_KEY=YOUR_API_KEY):

    # set api key
    openai.api_key = API_KEY

    # Call the chat GPT API
    completion = openai.Completion.create(
        # 'text-curie-001'  # 'text-babbage-001' #'text-ada-001'  #ëª¨ë¸ ì¢…ë¥˜ë³„ ê¸°ëŠ¥ í™•ì¸í•´ë³´ê¸°
        engine='text-davinci-003', prompt=prompt, temperature=0.5, max_tokens=1024, top_p=1, frequency_penalty=0, presence_penalty=0)

    return completion['choices'][0]['text']


# chatGPTê°€ ì •ìƒì‹¤í–‰ë  ìˆ˜ ìˆë„ë¡ í˜ì´ì§€ì— ì²˜ìŒ ì ‘ì†ì‹œ ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì½”ë“œ - ì•ˆì“¸ê±°ì„
def chatGPT_execution_confirmation():
  st.write("chatGPT ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ì¤‘ ...")
  prompt = f"this is running test."
  answer = chatGPT(prompt).strip()
  print("answer : ", answer)

  if answer is not None:
    st.write("ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ì™„ë£Œ.")

  else:
    st.write("ë¹„ì •ìƒ ì‘ë™")

# ë„¤ì´ë²„ ê²€ìƒ‰ìš©) í¬ìŠ¤íŠ¸ í¬ë¡¤ëŸ¬
def naver_post_crawler_for_naver_search(url):
    st.write("ë„¤ì´ë²„í¬ìŠ¤íŠ¸ ë§í¬ì…ë‹ˆë‹¤.")

    playwright = sync_playwright().start() # í¬ë¡¤ë§ìš© í”Œë ˆì´ë¼ì´íŠ¸ ì‹œì‘
    browser = playwright.chromium.launch(headless=True) 
    page = browser.new_page() # ì›¹ë¸Œë¼ìš°ì € ì—´ê¸°
    page.goto(url) # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ url ì£¼ì†Œë¡œ ì´ë™
    ctx = page.locator('xpath=/html/body') # ì˜¤í”ˆí•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ html code path ì§€ì •(bodyë¡œ ì§€ì •í•´ì„œ ì „ì²´ ê°€ì ¸ì˜´)
    ctx_inner_html = ctx.inner_html() # ì¡°íšŒí•œ í˜ì´ì§€ì—ì„œ html ì½”ë“œ ê°€ì ¸ì˜¤ê¸°

    # í¬ìŠ¤íŠ¸ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    # ì œëª© íƒœê·¸ëª…ì„ ì„ íƒí•œ ë’¤ class ì´ë¦„ ë¶™ì—¬ì„œ ì ‘ê·¼
    title_tag = page.locator('h3.se_textarea')
    title_text = title_tag.inner_html().replace("  ","")
    st.write("ê²Œì‹œê¸€ ì œëª©:", title_text)

    # ì•ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ title íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì‹œì‘ëœë‹¤(ì œëª©, ë³¸ë¬¸)
    under_title_tag_location = ctx_inner_html.find("<h3 class=\"se_textarea")
    

    # ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ end íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ê°€ ìœ„ì¹˜í•œ ê³³ë¶€í„° ë‚´ìš©ì„ ì˜ë¼ë‚¸ë‹¤(ë³¸ë¬¸ì´ ëë‚œ ì´í›„ ë°”ë¡œ ë‹¤ìŒì— ë“±ì¥í•˜ëŠ” íƒœê·¸ë¡œ ì§€ì •)
    over_end_tag_location = ctx_inner_html.find("<div class=\"state_line") 

    # ì¡°íšŒí•œ íƒœê·¸ì˜ ìœ„ì¹˜ë§Œí¼ html ì½”ë“œë¥¼ ì˜ë¼ë‚´ê¸°
    tag_data = ctx_inner_html[under_title_tag_location:over_end_tag_location]

    # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    cleantext = BeautifulSoup(tag_data, "lxml").text
    naver_post_cleantext = cleantext.replace("\n\n", "")
    # print(cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
    st.write("ë³¸ë¬¸:")
    st.write(naver_post_cleantext)

    browser.close()
    print("-------------------ì™„ë£Œ------------------")

    return naver_post_cleantext


# êµ¬ê¸€ ê²€ìƒ‰ìš©) ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ í¬ë¡¤ëŸ¬
def naver_post_crawler_for_google_search(url, playwright, browser):
    st.write("ë„¤ì´ë²„í¬ìŠ¤íŠ¸ ë§í¬ì…ë‹ˆë‹¤.")

    # playwright = sync_playwright().start() # í¬ë¡¤ë§ìš© í”Œë ˆì´ë¼ì´íŠ¸ ì‹œì‘ - ì¤‘ë³µì„ ì–¸ ë¶ˆê°€
    browser = playwright.chromium.launch(headless=True) 
    page = browser.new_page() # ì›¹ë¸Œë¼ìš°ì € ì—´ê¸°
    page.goto(url) # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ url ì£¼ì†Œë¡œ ì´ë™
    ctx = page.locator('xpath=/html/body') # ì˜¤í”ˆí•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ html code path ì§€ì •(bodyë¡œ ì§€ì •í•´ì„œ ì „ì²´ ê°€ì ¸ì˜´)
    ctx_inner_html = ctx.inner_html() # ì¡°íšŒí•œ í˜ì´ì§€ì—ì„œ html ì½”ë“œ ê°€ì ¸ì˜¤ê¸°

    # í¬ìŠ¤íŠ¸ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    # ì œëª© íƒœê·¸ëª…ì„ ì„ íƒí•œ ë’¤ class ì´ë¦„ ë¶™ì—¬ì„œ ì ‘ê·¼
    title_tag = page.locator('h3.se_textarea')
    title_text = title_tag.inner_html().replace("  ","")
    st.write("ê²Œì‹œê¸€ ì œëª©:", title_text)

    # ì•ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ title íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì‹œì‘ëœë‹¤(ì œëª©, ë³¸ë¬¸)
    under_title_tag_location = ctx_inner_html.find("<h3 class=\"se_textarea")

    # ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ end íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ê°€ ìœ„ì¹˜í•œ ê³³ë¶€í„° ë‚´ìš©ì„ ì˜ë¼ë‚¸ë‹¤(ë³¸ë¬¸ì´ ëë‚œ ì´í›„ ë°”ë¡œ ë‹¤ìŒì— ë“±ì¥í•˜ëŠ” íƒœê·¸ë¡œ ì§€ì •)
    over_end_tag_location = ctx_inner_html.find("<div class=\"state_line") 

    # ì¡°íšŒí•œ íƒœê·¸ì˜ ìœ„ì¹˜ë§Œí¼ html ì½”ë“œë¥¼ ì˜ë¼ë‚´ê¸°
    tag_data = ctx_inner_html[under_title_tag_location:over_end_tag_location]

    # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    cleantext = BeautifulSoup(tag_data, "lxml").text
    naver_post_cleantext = cleantext.replace("\n\n", "")
    # print(cleantext.replace("\n\n", "")) # ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
    st.write("ë³¸ë¬¸:")
    st.write(naver_post_cleantext)

    browser.close()
    print("-------------------ì™„ë£Œ------------------")

    return naver_post_cleantext
  
# ë¸”ë¡œê·¸ ë³¸ë¬¸ì€ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ iframe ì œê±° í›„ blog.naver.com ë¶™ì´ê¸°
def delete_iframe(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    src_url = "https://blog.naver.com/" + soup.iframe["src"]

    return src_url

# ë„¤ì´ë²„ë¸”ë¡œê·¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ - ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0, ONE í¬í•¨
def text_scraping(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    if soup.find("div", attrs={"class": "se-main-container"}): # ë³¸ë¬¸ ì „ì²´ íƒœê·¸
        text = soup.find(
            "div", attrs={"class": "se-main-container"}).get_text()
        text = text.replace("\n", "") 
        return text

    elif soup.find("div", attrs={"id": "postViewArea"}):  # ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0ë„ í¬ë¡¤ë§ í•´ì˜¤ê¸° ìœ„í•¨
        text = soup.find("div", attrs={"id": "postViewArea"}).get_text()
        text = text.replace("\n", "")
        return text

    else:
        return "ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ë§ì§€ë§Œ, í™•ì¸ì´ ë¶ˆê°€í•œ ë°ì´í„°ì…ë‹ˆë‹¤."  # ìŠ¤ë§ˆíŠ¸ ì—ë””í„°ë¡œ ì‘ì„±ë˜ì§€ ì•Šì€ ê¸€ì€ ì¡°íšŒí•˜ì§€ ì•ŠìŒ ..


# ë„¤ì´ë²„ ê²€ìƒ‰ìš©)
# ë„¤ì´ë²„ì— ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì—¬ View íƒ­ìœ¼ë¡œ ì´ë™, liì˜ linkë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def naver_crawling(input_data):

  crawling_done_count = 0
  original_crawling_result =""

  # ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥
  query = input_data
  url = "https://search.naver.com/search.naver?where=view&sm=tab_jum&query=" + \
      quote(query)  # ì•„ìŠ¤í‚¤ì½”ë“œí˜•ì‹ìœ¼ë¡œ ë³€í™˜, ë„¤ì´ë²„ View íƒ­ ê¸°ì¤€ìœ¼ë¡œ ê¸ì–´ì˜¤ê¸°

  # ë‚´ í—¤ë” : Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36
  headers = {
      "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
  res = requests.get(url, headers=headers)
  res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
  soup = BeautifulSoup(res.text, "lxml")

  # ë„¤ì´ë²„ì—ì„œ ì •ì˜í•´ë†“ì€ ê²Œì‹œê¸€ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ íƒœê·¸ (limitìœ¼ë¡œ ì œí•œ, ìµœëŒ€ 30ê°œ ê°€ëŠ¥)
  # limitì€ ë‹¨ìˆœíˆ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜ ì œí•œì„
  posts = soup.find_all("li", attrs={"class": "bx _svp_item"}, limit=10) # li íƒœê·¸ì˜ bx _svp_itemì´ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ê°’ë“¤ì„ soupìœ¼ë¡œ ì°¾ì•„ì„œ ì €ì¥, limit 10 ì œí•œ
  
  
  for post in posts:  # posts ì…ë ¥ì‹œ 30ê°œ ê°€ì ¸ì˜´
    
    if crawling_done_count < 3:

        # í•´ë‹¹ ê²Œì‹œê¸€ì˜ ë§í¬ ê°€ì ¸ì˜¤ê¸°
        # ê°€ì ¸ì˜¬ íƒœê·¸ì™€ íƒœê·¸ì— ì‚¬ìš©ëœ classëª… ì…ë ¥í•´ì¤Œ
        post_link = post.find("a", attrs={"class": "api_txt_lines total_tit _cross_trigger"})[
            'href']
        st.write("ê²€ìƒ‰ëœ link : ", post_link)

        # í•´ë‹¹ ê²Œì‹œê¸€ì˜ ì œëª© ê°€ì ¸ì˜¤ê¸°
        post_title = post.find("a", attrs={
                                "class": "api_txt_lines total_tit _cross_trigger"}).get_text()  # í•´ë‹¹ ê²Œì‹œê¸€ì˜ ì œëª© text ê°’ ê°€ì ¸ì˜¤ê¸°
        st.write("ê²Œì‹œê¸€ ì œëª© : ", post_title)

        
        # ë„ë©”ì¸ë³„ ë¶„ë¦¬
        blog_p = re.compile("blog.naver.com")  # compile() : ë¬¸ìì—´ì„ ì»´íŒŒì¼í•´ì„œ íŒŒì´ì¬ ì½”ë“œë¡œ ë°˜í™˜
        blog_m = blog_p.search(post_link)
        cafe_p = re.compile("cafe.naver.com") # ì¹´í˜ ê²Œì‹œê¸€ ê±°ë¥´ê¸° ìœ„í•¨
        cafe_m = cafe_p.search(post_link)
        post_p = re.compile("post.naver.com")
        post_m = post_p.search(post_link)
        
        # urlì— blogê°€ í¬í•¨ëœ ê²½ìš°
        if blog_m:
            st.write("ë„¤ì´ë²„ ë¸”ë¡œê·¸ì…ë‹ˆë‹¤.")
            blog_text = text_scraping(delete_iframe(post_link))
            st.write("í¬ë¡¤ë§ëœ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ : ", blog_text)

            # í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë²ˆì—­(en ì´ì™¸ì˜ ì–¸ì–´ì¸ ê²½ìš°)
            for_prompt_data_one = google_translator(blog_text)

            st.write("chatGPT ì‹¤í–‰ì¤‘...")

            # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸ì™€ enìœ¼ë¡œ ë²ˆì—­ëœ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ chatGPT í”„ë¡¬í”„íŠ¸ì— ì „ì†¡
            prompt_command = input_prompt + " " + for_prompt_data_one # ì§ì ‘ input ëœ ë°ì´í„°ê°€ ë“¤ì–´ê°€ëŠ”ì§€ í™•ì¸ í•„ìš”
            st.write(input_prompt)
            # st.write("prompt_command:",prompt_command)
            result = chatGPT(prompt_command).strip()  # ì™¼ìª½, ì˜¤ë¥¸ìª½ ê³µë°± ì œê±°
            st.write("ê²°ê³¼ ì›ë¬¸:", result)
            st.write("ê²°ê³¼ ê¸¸ì´:", len(result))

            # chatGPT ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë©´ 3íšŒ ì¬ìš”ì²­
            retry_prompt(result, prompt_command)

            # chatGPTë¥¼ í†µí•´ ë°›ì€ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ë²ˆì—­
            result_to_kr = translator.translate(result, dest='ko')
            st.write("chatGPT ìš”ì²­ ê²°ê³¼(í•œêµ­ì–´ë¡œ ë²ˆì—­):", result_to_kr.text)

            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì™„ë£Œì‹œ count ì¦ê°€
            crawling_done_count = crawling_done_count + 1
            st.write("crawling_done_count :",crawling_done_count)

            st.write("-"*50)

        # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        elif post_m:
            # í¬ìŠ¤íŠ¸ ì „ìš© í¬ë¡¤ëŸ¬ ì‹¤í–‰
            original_crawling_result = naver_post_crawler_for_naver_search(post_link)

            # í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë²ˆì—­(en ì´ì™¸ì˜ ì–¸ì–´ì¸ ê²½ìš°)
            for_prompt_data_one = google_translator(original_crawling_result)

            st.write("chatGPT ì‹¤í–‰ì¤‘...")

            # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸ì™€ enìœ¼ë¡œ ë²ˆì—­ëœ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ chatGPT í”„ë¡¬í”„íŠ¸ì— ì „ì†¡
            prompt_command = input_prompt + " " + for_prompt_data_one
            # st.write("prompt_command:",prompt_command)
            result = chatGPT(prompt_command).strip()  # ì™¼ìª½, ì˜¤ë¥¸ìª½ ê³µë°± ì œê±°
            st.write("ê²°ê³¼ ì›ë¬¸:", result)
            st.write("ê²°ê³¼ ê¸¸ì´:", len(result))

            # chatGPT ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë©´ 3íšŒ ì¬ìš”ì²­
            retry_prompt(result, prompt_command)

            # chatGPTë¥¼ í†µí•´ ë°›ì€ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ë²ˆì—­
            result_to_kr = translator.translate(result, dest='ko')
            st.write("chatGPT ìš”ì²­ ê²°ê³¼(í•œêµ­ì–´ë¡œ ë²ˆì—­):", result_to_kr.text)

            # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œì‹œ count ì¦ê°€
            crawling_done_count = crawling_done_count + 1
            st.write("crawling_done_count :",crawling_done_count)

            st.write("-"*50)


        # ë„¤ì´ë²„ ì¹´í˜ì¸ ê²½ìš°
        elif cafe_m:
            st.write("ë„¤ì´ë²„ ì¹´í˜ëŠ” í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.") # ì ‘ê·¼ ê¶Œí•œ ë¬¸ì œë¡œ ì¸í•´ í¬ë¡¤ë§ ë¶ˆê°€

        else:
            st.write("ì´ í”„ë¡œê·¸ë¨ì—ì„œ í¬ë¡¤ë§ ê°€ëŠ¥í•œ ê¸€ì´ ì•„ë‹™ë‹ˆë‹¤.")
    else:
        
        st.write("ì •ìƒ í¬ë¡¤ë§ 3íšŒ ì™„ë£Œ.")
        break



# ë¸ŒëŸ°ì¹˜ ë§í¬ í¬ë¡¤ëŸ¬
# êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•´ì˜¤ëŠ” ê³¼ì •ì—ì„œ ì„ ì–¸í•œ browser ê·¸ëŒ€ë¡œ ì‚¬ìš©
def brunch_crawler(url, playwright, browser):
    st.write("ë¸ŒëŸ°ì¹˜ ë§í¬ì…ë‹ˆë‹¤.")

    # playwright = sync_playwright().start() # í¬ë¡¤ë§ìš© í”Œë ˆì´ë¼ì´íŠ¸ ì‹œì‘ - ì¤‘ë³µ ì„ ì–¸ ë¶ˆê°€
    browser = playwright.chromium.launch(headless=True) 
    page = browser.new_page() # ì›¹ë¸Œë¼ìš°ì € ì—´ê¸°
    page.goto(url) # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ url ì£¼ì†Œë¡œ ì´ë™
    ctx = page.locator('xpath=/html/body') # ì˜¤í”ˆí•œ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¬ html code path ì§€ì •(bodyë¡œ ì§€ì •í•´ì„œ ì „ì²´ ê°€ì ¸ì˜´)
    ctx_inner_html = ctx.inner_html() # ì¡°íšŒí•œ í˜ì´ì§€ì—ì„œ html ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
    # print(ctx_inner_html)

    # ë¸ŒëŸ°ì¹˜ ê²Œì‹œê¸€ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    # ì œëª© íƒœê·¸ëª…ì„ ì„ íƒí•œ ë’¤ class ì´ë¦„ ë¶™ì—¬ì„œ ì ‘ê·¼
    title_tag = page.locator('h1.cover_title')
    title_text = title_tag.inner_html().replace("  ","")
    st.write("ì œëª© : ", title_text)

    # ì•ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ ë¸ŒëŸ°ì¹˜ title íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ì˜ ë‚´ìš©ì´ ì‹œì‘ëœë‹¤(ì œëª©, ë³¸ë¬¸)
    under_title_tag_location = ctx_inner_html.find("<h1 class=\"cover_title")

    # ë’·ë¶€ë¶„ì„ ì˜ë¼ë‚´ê¸° ìœ„í•´ ë¸ŒëŸ°ì¹˜ end íƒœê·¸ ì§€ì •
    # í•´ë‹¹ íƒœê·¸ê°€ ìœ„ì¹˜í•œ ê³³ë¶€í„° ë‚´ìš©ì„ ì˜ë¼ë‚¸ë‹¤(ë³¸ë¬¸ì´ ëë‚œ ì´í›„ ë°”ë¡œ ë‹¤ìŒì— ë“±ì¥í•˜ëŠ” íƒœê·¸ë¡œ ì§€ì •)
    over_end_tag_location = ctx_inner_html.find("<div class=\"wrap_body_info") 

    # ì¡°íšŒí•œ íƒœê·¸ì˜ ìœ„ì¹˜ë§Œí¼ html ì½”ë“œë¥¼ ì˜ë¼ë‚´ê¸°
    tag_data = ctx_inner_html[under_title_tag_location:over_end_tag_location]

    # ì˜ë¼ë‚¸ html ì½”ë“œì—ì„œ tag ë°ì´í„°ë¥¼ ì§€ìš°ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ê¸°
    cleantext = BeautifulSoup(tag_data, "lxml").text
    brunch_cleantext = cleantext.replace("\n\n", "")# ë¶ˆí•„ìš”í•œ ê³µë°±(ì—”í„°) ì œê±° í›„ ì¶œë ¥í™•ì¸
    st.write("ë³¸ë¬¸ : ")
    st.write(brunch_cleantext)

    browser.close()
    print("-------------------ì™„ë£Œ------------------")

    return brunch_cleantext

# êµ¬ê¸€ê²€ìƒ‰ìš©) ë„¤ì´ë²„ ë¸”ë¡œê·¸ - iframe ì œê±° í›„ blog.naver.com ë¶™ì´ê¸°
# def naver_blog_delete_iframe(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    src_url = "https://blog.naver.com/" + soup.iframe["src"]

    return src_url

# êµ¬ê¸€ê²€ìƒ‰ìš©) ë„¤ì´ë²„ë¸”ë¡œê·¸ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ - ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0, ONE í¬í•¨
# def naver_blog_text_scraping(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
    res = requests.get(url, headers=headers)
    res.raise_for_status()  # ë¬¸ì œì‹œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ
    soup = BeautifulSoup(res.text, "lxml")

    if soup.find("div", attrs={"class": "se-main-container"}): # ë³¸ë¬¸ ì „ì²´ íƒœê·¸
        text = soup.find(
            "div", attrs={"class": "se-main-container"}).get_text()
        text = text.replace("\n", "") 
        return text

    elif soup.find("div", attrs={"id": "postViewArea"}):  # ìŠ¤ë§ˆíŠ¸ ì—ë””í„° 2.0ë„ í¬ë¡¤ë§ í•´ì˜¤ê¸° ìœ„í•¨
        text = soup.find("div", attrs={"id": "postViewArea"}).get_text()
        text = text.replace("\n", "")
        return text


    else:
        return "ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ë§ì§€ë§Œ, í™•ì¸ì´ ë¶ˆê°€í•œ ë°ì´í„°ì…ë‹ˆë‹¤."  # í¬ë¡¤ë§ ì•ˆë˜ëŠ” ê¸€ ì²˜ë¦¬

# êµ¬ê¸€ê²€ìƒ‰ìš©) ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§í•´ì„œ ì¶œë ¥
def naver_blog_crawler(url):
    st.write("ë„¤ì´ë²„ë¸”ë¡œê·¸ ë§í¬ì…ë‹ˆë‹¤.")

    blog_text = text_scraping(delete_iframe(url))
    print("ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ : ", blog_text)
    st.write(blog_text)

    print("-------------------ì™„ë£Œ------------------")

    return blog_text


# newspaper ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ëŸ¬
def newspaper_crawler(url):
    # ì…ë ¥ë°›ì€ urlì„ í†µí•´ í¬ë¡¤ë§ í•´ì˜¤ê¸° ìœ„í•œ ì½”ë“œ - newspaper Article ì‚¬ìš©
    url_str = str(url)
    url_strip = url_str.strip()
    page = Article(url_strip, config=config) # ì–¸ì–´ì„¤ì •ì„ í•˜ì§€ ì•Šìœ¼ë©´ ìë™ê°ì§€ëœë‹¤.
    page.download()
    page.parse() # ì›¹í˜ì´ì§€ parse ì§„í–‰

    # í¬ë¡¤ë§ ëœ ê²°ê³¼ì˜ ë³¸ë¬¸(text)ì„ ë³€ìˆ˜ì— ë”°ë¡œ ì €ì¥
    url_crawling_text = page.text
    url_crawling_title = page.title

    # í¬ë¡¤ë§ ì›ë³¸ ê²°ê³¼ ì¶œë ¥
    st.write("ê²Œì‹œê¸€ ì œëª© : ", url_crawling_title)
    st.write("í¬ë¡¤ë§ ê²°ê³¼ : ")
    st.write(url_crawling_text)

    return url_crawling_text


# ìœ íŠœë¸Œ ë§í¬ì¸ ê²½ìš° í¬ë¡¤ë§ í•´ì£¼ëŠ” í•¨ìˆ˜
def youtube_script_crawling(id):
    st.write("ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í¬ë¡¤ë§ ì‹¤í–‰ì¤‘...")

    # urlê³¼ video idë¥¼ í†µí•´ ì œëª©ì„ í¬ë¡¤ë§í•´ì˜¬ ë™ì˜ìƒ ì£¼ì†Œì— ì ‘ê·¼
    params = {"format": "json", "url": "https://www.youtube.com/watch?v=%s" % id}
    url = "https://www.youtube.com/oembed"
    query_string = urllib.parse.urlencode(params)
    url = url + "?" + query_string

    # ë™ì˜ìƒ titleì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    with urllib.request.urlopen(url) as response:
        response_text = response.read()
        data = json.loads(response_text.decode())
        video_title = data['title']
        # ì‘ë‹µë°›ì€ ê°’ì—ì„œ ì œëª© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        st.write("ì˜ìƒ ì œëª© : ", video_title)

    # í”„ë¡œê·¸ë¨ ì‹œì‘
    srt1 = YouTubeTranscriptApi.get_transcript(id, languages=['ko'])

    text = ''

    for i in range(len(srt1)):
        text += srt1[i]['text'] + ' '  # text ë¶€ë¶„ë§Œ ê°€ì ¸ì˜´

    # ê³µë°± ì œê±°
    text_ = text.replace(' ', '')

    kkma = Kkma()
    text_sentences = kkma.sentences(text_)

    # ë¬¸ì¥ì´ ëë‚˜ëŠ” ì–´ì ˆì„ ì„ì˜ë¡œ ì§€ì •í•´ì„œ ë¶„ë¦¬í•  ìš©ë„ë¡œ ì‚¬ìš©
    lst = ['ì£ ', 'ë‹¤', 'ìš”', 'ì‹œì˜¤', 'ìŠµë‹ˆê¹Œ', 'ì‹­ë‹ˆê¹Œ', 'ë©ë‹ˆê¹Œ', 'ì˜µë‹ˆê¹Œ', 'ë­¡ë‹ˆê¹Œ',]

    # ì œì™¸í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ íŒŒì¼ì„ ì½ì–´ë“¤ì—¬ì„œ forë¬¸ì— ì‚¬ìš©
    df = pd.read_csv('not2.csv', encoding='utf-8')
    not_verb = df.loc[:, 'col1'].to_list()

    text_all = ' '.join(text_sentences).split(' ')

    for n in range(len(text_all)):
        i = text_all[n]
        if len(i) == 1:  # í•œê¸€ìì¼ ê²½ìš° ì¶”ê°€ë¡œ ì‘ì—…í•˜ì§€ ì•ŠìŒ
            continue

        else:
            for j in lst:  # ì¢…ê²° ë‹¨ì–´
                # ì§ˆë¬¸í˜•
                if j in lst[4:]:
                    i += '?'

                # ëª…ë ¹í˜•
                elif j == 'ì‹œì˜¤':
                    i += '!'

                # ë§ˆì¹¨í‘œ
                else:
                    if i in not_verb:  # íŠ¹ì • ë‹¨ì–´ ì œì™¸
                        continue

                    else:
                        if j == i[len(i)-1]:  # ì¢…ê²°
                            text_all[n] += '.'
                            # print("\n", text_all[n], end='/ ')

    # ë‚˜ë‰œ ë¬¸ì¥ì„ ë‹¤ì‹œ ì¡°ì¸
    spacing = Spacing()
    text_all_in_one = ' '.join(text_all)
    result = spacing(text_all_in_one.replace(' ', ''))
    st.write(result) # ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„° ì¶œë ¥

    return result


# í¬ë¡¤ë§ëœ ê²°ê³¼ì˜ ì–¸ì–´ë¥¼ ìë™ ê°ì§€í•˜ê³  ë²ˆì—­í•´ì£¼ëŠ” í•¨ìˆ˜
def google_translator(original_crawling_data):
    # detect ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ ìë™ê°ì§€
    language_sensing = detect(original_crawling_data)
    st.write("ì–¸ì–´ê°ì§€ : ", language_sensing)

    # í¬ë¡¤ë§í•œ ì–¸ì–´ê°€ í•œêµ­ì–´, ì¼ë³¸ì–´ì´ë©´ êµ¬ê¸€ ë²ˆì—­ê¸° ì‹¤í–‰(í•œê¸€,ì¼ë³¸ì–´ë§Œ -> ì˜ì–´)
    if language_sensing == 'ko' or language_sensing == 'ja':
        url_crawling_slice = original_crawling_data[0:5000] # êµ¬ê¸€ë²ˆì—­ê¸° ìµœëŒ€ ì œí•œìˆ˜ë¡œ ì¸í•œ ìŠ¬ë¼ì´ìŠ¤

        # ì˜ì–´ë¡œ ë²ˆì—­
        crawling_data_translate = translator.translate(url_crawling_slice, dest='en')

        # í”„ë¡¬í”„íŠ¸ì— ë‚ ë¦¬ê¸° ìœ„í•´ ì •ë¦¬ëœ í¬ë¡¤ë§ ë°ì´í„° ë³€ìˆ˜ì— ì €ì¥
        for_prompt_data = crawling_data_translate.text

        # ë²ˆì—­ëœ ê²°ê³¼ ì¶œë ¥
        st.write("----- url1 í¬ë¡¤ë§ ê²°ê³¼ ë²ˆì—­(en) -----")
        st.write(for_prompt_data)

    # í¬ë¡¤ë§í•œ ì–¸ì–´ê°€ ì˜ì–´ë©´ ë²ˆì—­ ì—†ì´ ê·¸ëŒ€ë¡œ ì§„í–‰ - ê¸´ ê²½ìš° ìŠ¬ë¼ì´ìŠ¤ë§Œ
    elif language_sensing == 'en':
        for_prompt_data = original_crawling_data[0:10000] # chatgpt ìµœëŒ€ ì œí•œìˆ˜ë¡œ ì¸í•œ ìŠ¬ë¼ì´ìŠ¤
    
    # í¬ë¡¤ë§ ë˜ì§€ ì•Šì€ ê²½ìš° == ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° -> ì—ëŸ¬ì²˜ë¦¬
    else:
        st.error("í¬ë¡¤ë§ëœ ê°’ì´ ë„ˆë¬´ ì ê±°ë‚˜ ëª¨í˜¸í•´ì„œ ì–¸ì–´ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ urlì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", icon="ğŸš¨")


    return for_prompt_data

def retry_prompt(answer, prompt):
    # ì‘ë‹µëœ ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë©´ (ì—†ìœ¼ë©´) 3íšŒê¹Œì§€ ë‹¤ì‹œ ì‹œë„
    if len(answer) == 0:
        st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.write("5ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (1/3).")
        time.sleep(5)

        first_re_answer = chatGPT(prompt).strip()
        st.write("chatGPT 1ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(first_re_answer))
        st.write("chatGPT 1ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", first_re_answer)

        if len(first_re_answer) == 0:
            st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.write("10ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (2/3).")
            time.sleep(10)

            second_re_answer = chatGPT(prompt).strip()
            st.write("chatGPT 2ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(second_re_answer))
            st.write("chatGPT 2ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", second_re_answer)

            if len(second_re_answer) == 0:
                st.write("[Error] chatGPTì˜ ë‹µë³€ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                st.write("15ì´ˆ í›„ ìë™ìœ¼ë¡œ ì¬ìš”ì²­ë©ë‹ˆë‹¤. (3/3).")
                time.sleep(15)

                third_re_answer = chatGPT(prompt).strip()
                st.write("chatGPT 3ì°¨ ì¬ìš”ì²­ ê²°ê³¼ì˜ ê¸¸ì´ : ", len(third_re_answer))
                st.write("chatGPT 3ì°¨ ì¬ìš”ì²­ ê²°ê³¼: ", third_re_answer)

                if len(third_re_answer) == 0:
                    st.write("[Error] ì ì‹œ í›„ ì¬ìš”ì²­ í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.")


# êµ¬ê¸€ ê²€ìƒ‰ì‹œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜
def google_search_crawling(input_data):
  crawling_done_count = 0
  original_crawling_result =""
  st.write("êµ¬ê¸€ í¬ë¡¤ë§ ì‹¤í–‰ì¤‘...")

  # êµ¬ê¸€ ê²€ìƒ‰ì‹œì˜ ê²Œì‹œê¸€ë“¤ì˜ url ê°’ ê°€ì ¸ì˜¤ê¸°
  baseUrl = 'https://www.google.com/search?q='
  # plusUrl = input_data
  # url = baseUrl + quote(plusUrl)
  # ê²€ìƒ‰ì–´ì™€ base url í•©ì¹˜ê¸°
  url = baseUrl + input_data 
  # í•œê¸€ì€ ì¸í„°ë„·ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¼, quote_plusê°€ ë³€í™˜í•´ì¤Œ -> URLì— %CE%GD%EC ì´ëŸ° ê±° ë§Œë“¤ì–´ì¤Œ

  #  í”Œë ˆì´ë¼ì´íŠ¸ë¡œ ë³€ê²½
  playwright = sync_playwright().start()
  browser = playwright.chromium.launch(headless=True, channel="chrome") 
  context = browser.new_context() # ì‹œí¬ë¦¿
  page = context.new_page() # ë¸Œë¼ìš°ì € ì‹¤í–‰í•˜ê¸°
  page.goto(url) # ì…ë ¥í•œ ê²€ìƒ‰ì–´ì˜ ê²€ìƒ‰ê²°ê³¼ íƒ­ìœ¼ë¡œ ì´ë™
  print("\n------------------------------------------------\n")
  print(page.content())
  html = page.content() 
  soup = BeautifulSoup(html, "lxml") # êµ¬ê¸€ ê²€ìƒ‰ê²°ê³¼ í™”ë©´ì˜ html ì½”ë“œ ê°€ì ¸ì™€ì„œ ì €ì¥

  # ì…€ë ‰íŠ¸í•  classëª… ì§€ì •
  # ê²€ìƒ‰ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì¹­í•˜ëŠ” class : MjjYud -> ê´‘ê³ íƒœê·¸ëŠ” ì œì™¸í•œ ëª¨ë“  ê²Œì‹œê¸€ì„ ê¸ì–´ì˜´ (ì´ë¯¸ì§€, ì˜ìƒ í¬í•¨ì¸ë“¯) - í™•ì¸ í•„ìš”
  google_search_result_list = soup.select('.MjjYud')[:10] # ìƒìœ„ 3ê°œ ê°€ì ¸ì˜´ - limitê³¼ ë¹„ìŠ· ..


  # ë™ì˜ìƒ -> X5OiLe ì•ˆì˜ hrefê°’ ê°€ì ¸ì™€ì•¼í•¨
  # ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ MjjYudë¡œ ê°€ì ¸ì˜¤ë©´ #fpstate=ive&vld=cid:~~í˜•íƒœì˜ ê°’ì´ hrefì— ë“¤ì–´ìˆìŒ
  # ì„ì‹œë°©í¸ìœ¼ë¡œ ... #fpstate=ive&vld=cid:ë¼ëŠ” ê¸€ìê°€ ìˆìœ¼ë©´ ë‹¤ì‹œ ì…€ë ‰íŠ¸í•´ì„œ hrefë¥¼ ê°€ì ¸ì˜¤ë„ë¡ ìˆ˜ì •
    # browser.close() # ì¢…ë£Œ í›„ close í•„ìš”

  # ê²Œì‹œê¸€ì˜ listë¥¼ ë°˜ë³µ ìˆœíšŒí•˜ë©° ì œëª© íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ê°’ê³¼ url ê°’ì„ ì¶œë ¥í•¨
  for i in google_search_result_list:


    # ì •ìƒì ìœ¼ë¡œ í¬ë¡¤ë§ ëœ íšŸìˆ˜ê°€ 3íšŒê¹Œì§€ ê°€ëŠ¥í•˜ë„ë¡ ì¡°ê±´ë¬¸ ì„¤ì •
    if crawling_done_count < 3:

        # url ê°’ë§Œ ê°€ì ¸ì˜¤ê¸°
        print(" êµ¬ê¸€ url : ", i.a.attrs['href'])
        st.write("êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ url : ", i.a.attrs['href'])
        link = i.a.attrs['href']

        if "https" not in link:
            st.write("í¬ë¡¤ë§ ê°€ëŠ¥í•œ ë§í¬ê°€ ì•„ë‹™ë‹ˆë‹¤. (ê²Œì‹œê¸€ì´ ì•„ë‹˜)")
            continue
        
        # ë™ì˜ìƒ ë§í¬ ì¸ì‹í•˜ê¸°
        # ì—ëŸ¬ê°€ ë‚¬ë‹¤ê°€ ë§ì•˜ë‹¤ê°€ í•¨
        elif '#fpstate=ive&vld=cid:' in link:
            video_list = soup.select('.X5OiLe')[:1] # ì¼ë‹¨ í•œê°œë§Œ ê°€ì ¸ì˜¤ê²Œ ì„¤ì •

            for i in video_list:
                link = i.a.attrs['href']
                st.write("êµ¬ê¸€ url(video) : ", i.a.attrs['href'])

        # url domain ê°’ìœ¼ë¡œ ì–´ë–¤ ì‚¬ì´íŠ¸ì¸ì§€ íŒë‹¨
        # 1. ìœ íŠœë¸Œ urlì¸ ê²½ìš°
        elif 'youtube' in link:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            st.write("ìœ íŠœë¸Œ ë§í¬ì…ë‹ˆë‹¤.")
            youtube_url = link
            search_value = "v="
            index_no = youtube_url.find(search_value)
            video_id = youtube_url[index_no+2:]

            print("video_id:",video_id)
            st.write("video_id:",video_id)

            # ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = youtube_script_crawling(video_id)

        # 2. ë¸ŒëŸ°ì¹˜ì¸ ê²½ìš°
        elif 'brunch' in link:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë¸ŒëŸ°ì¹˜ ê¸€ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = brunch_crawler(link, playwright, browser)

        # 3. ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš°
        elif 'blog.naver.com' in link:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë„¤ì´ë²„ë¸”ë¡œê·¸ ê¸€ì„ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = naver_blog_crawler(link)

        # 4. ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ì¸ ê²½ìš°
        elif 'post.naver.com' in link:
            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")
            # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ê¸€ì„ í¬ë¡¤ë§ í›„ ì›ë³¸ ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— ì €ì¥
            original_crawling_result = naver_post_crawler_for_google_search(link, playwright, browser)

        # 5. ìˆ˜ë™ìœ¼ë¡œ ì œì™¸í•  ì‚¬ì´íŠ¸ ëª…ì‹œí•˜ê¸° - í˜„ì¬ëŠ” ë‚˜ë¬´ìœ„í‚¤ë§Œ ì§€ì •
        elif 'namu' in link:
            st.write("í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ë§í¬ì…ë‹ˆë‹¤. (ë‚˜ë¬´ìœ„í‚¤)")
            continue

        # 6. ì´ì™¸ì˜ ê²½ìš°
        else:

            # í™•ì¸ìš© ì•ˆë‚´ë¬¸êµ¬
            st.write("ê¸°íƒ€ ë§í¬ì…ë‹ˆë‹¤.")
            st.write("í¬ë¡¤ë§ ì‘ì—… ì§„í–‰ì¤‘...")

            original_crawling_result = newspaper_crawler(link)


        if len(original_crawling_result) == 0:
            st.write("í¬ë¡¤ë§ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì£¼ì†Œì…ë‹ˆë‹¤.") # ì—¬ê¸°ë‹¤ê°€ ê·¸ëƒ¥ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ëª¨ë‘ ê¸ì–´ì˜¤ëŠ” ì½”ë“œ ì¶”ê°€í•˜ê¸°
            continue
            
        # í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë²ˆì—­(en ì´ì™¸ì˜ ì–¸ì–´ì¸ ê²½ìš°)
        for_prompt_data_one = google_translator(original_crawling_result)

        st.write("chatGPT ì‹¤í–‰ì¤‘...")

        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ëª…ë ¹ì–´(prompt)ì™€ sliceí•œ string ê°’ì„ ë¶™ì—¬ì„œ chatGPTì— ë‚ ë¦¬ëŠ” ì‘ì—…
        prompt_command = input_prompt + " " + for_prompt_data_one
        # st.write(input_prompt)
        # st.write("prompt_command:",prompt_command)
        result = chatGPT(prompt_command).strip()  # ì™¼ìª½, ì˜¤ë¥¸ìª½ ê³µë°± ì œê±°
        st.write("ê²°ê³¼ ì›ë¬¸:", result)
        st.write("ê²°ê³¼ ê¸¸ì´:", len(result))

        # chatGPT ê²°ê³¼ì˜ ê¸¸ì´ê°€ 0ì´ë©´ 3íšŒ ì¬ìš”ì²­
        retry_prompt(result, prompt_command)

        # chatGPTë¥¼ í†µí•´ ë°›ì€ ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ë‹¤ì‹œ ë²ˆì—­
        result_to_kr = translator.translate(result, dest='ko')
        st.write("chatGPT ìš”ì²­ ê²°ê³¼(í•œêµ­ì–´ë¡œ ë²ˆì—­):", result_to_kr.text)
        
        crawling_done_count = crawling_done_count + 1
        st.write("crawling_done_count :",crawling_done_count)

        st.write("-"*50)

    else:
        st.write("í¬ë¡¤ë§ 3íšŒ ì™„ë£Œ.")
        break

# chatGPTì— ìš”ì²­ì„ ë‹¤ì‹œ ë‚ ë¦¬ëŠ” ì¬ìš”ì²­ ë²„íŠ¼ - ì•ˆì”€
def chatGPT_retry_button(prompt):

  disabled_flag = True
  time.sleep(5)
  disabled_flag = False

  if st.button("ì¬ìš”ì²­", disabled=disabled_flag):
    
    re_answer = chatGPT(prompt).strip()
    st.write("chatGPT ì¬ìš”ì²­ ê²°ê³¼ : ", re_answer)
  
  else:
      st.write(" ")


# ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤í–‰ìš© í•¨ìˆ˜ - ë²„íŠ¼ í´ë¦­ì‹œ ì‘ë™
def start_naver_search_button(input_data):
    # ì…ë ¥ê°’ì´ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ë²„íŠ¼ ë¹„í™œì„±í™”
    disabled_flag = True

    # ì…ë ¥ê°’ì´ ìƒê¸°ë©´ ë²„íŠ¼ í™œì„±í™”
    if len(input_data) != 0:
        disabled_flag = False

    # ë²„íŠ¼ í´ë¦­ì‹œ ì‹¤í–‰
    if st.button("ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹œì‘", disabled=disabled_flag):
      st.write("ë„¤ì´ë²„ ì‹œì‘")

      naver_crawling(input_data)

      st.write("ì™„ë£Œ")

    else:
      st.write(" ")

# êµ¬ê¸€ í¬ë¡¤ë§ ì‹¤í–‰ìš© í•¨ìˆ˜ - ë²„íŠ¼ í´ë¦­ì‹œ ì‘ë™
def start_google_search_button(input_data):
    # ì…ë ¥ê°’ì´ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ë²„íŠ¼ ë¹„í™œì„±í™”
    disabled_flag = True

    # ì…ë ¥ê°’ì´ ìƒê¸°ë©´ ë²„íŠ¼ í™œì„±í™”
    if len(input_data) != 0:
        disabled_flag = False

    # ë²„íŠ¼ í´ë¦­ì‹œ ì‹¤í–‰
    if st.button("êµ¬ê¸€ í¬ë¡¤ë§ ì‹œì‘", disabled=disabled_flag):
      st.write("êµ¬ê¸€ ì‹œì‘")

      google_search_crawling(input_data)

      
      st.write("ì™„ë£Œ")

    else:
      st.write(" ")



# ì œëª©
st.title('ê²€ìƒ‰ì—”ì§„ì— ê²€ìƒ‰í•œ ê²°ê³¼ë¥¼ í¬ë¡¤ë§í•´ì„œ chatGPTì— ëª…ë ¹í•˜ê¸°')

# ì•ˆë‚´ ë¬¸êµ¬
# chatGPT_execution_confirmation()
# st.info('í¬ë¡¤ë§ ê°€ëŠ¥ ì–¸ì–´ : í•œêµ­ì–´(ko), ì˜ì–´(en), ì¼ë³¸ì–´(ja) / ìë™ì¸ì‹ë¨', icon="â„¹ï¸")

# í…ìŠ¤íŠ¸
st.write("ê¸°ëŠ¥1) ë„¤ì´ë²„ì— ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
input_naver = st.text_input(label='ë„¤ì´ë²„ ê²€ìƒ‰')
st.write("ì…ë ¥í™•ì¸ : ", input_naver)
st.write("")
st.write("ê¸°ëŠ¥2) êµ¬ê¸€ì— ê²€ìƒ‰í•  ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
input_google = st.text_input(label='êµ¬ê¸€ ê²€ìƒ‰')
st.write("ì…ë ¥í™•ì¸ : ", input_google)
st.write("")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
st.write("ChatGPTì— ìš”ì²­í•  promptë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ê¸°ë³¸ê°’ : Summarize the following)")
input_prompt = st.text_input(label='ex) Summarize the following ...', value="Summarize the following ")
st.write("ì…ë ¥í™•ì¸ : ", input_prompt)

# í¬ë¡¤ë§ ì‹¤í–‰ ë²„íŠ¼
start_naver_search_button(input_naver)
start_google_search_button(input_google)


